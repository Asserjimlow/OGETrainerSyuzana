Глава 4. Кодирование символов

Кодирование символов — это процесс сопоставления каждому символу определённого числового кода. Это необходимо для хранения и обработки текста на компьютере.

Основные стандарты кодировки:

1. **ASCII (American Standard Code for Information Interchange)**
   - Использует 7 бит (128 символов), позже расширен до 8 бит (256 символов — расширенный ASCII).
   - Поддерживает английский алфавит, цифры, знаки препинания и служебные символы.
   - Пример: 'A' = 65, 'a' = 97, '0' = 48

2. **Unicode**
   - Поддерживает все языки мира (включая русский, китайский и т.д.)
   - Существует несколько реализаций: UTF-8, UTF-16, UTF-32
   - UTF-8 — самая распространённая кодировка в интернете, использует от 1 до 4 байт на символ.

Сравнение ASCII и Unicode:
| Символ | ASCII (двоичный) | Unicode (UTF-8) |
|--------|------------------|-----------------|
| A      | 01000001         | 01000001        |
| Я      | —                | 11011010 10100000 10111000 10000001 (UTF-8) |

Проблема несовместимости кодировок:
До появления Unicode в разных странах использовались свои локальные кодировки (например, Windows-1251 для русского языка).
Это приводило к "кракозябрам", если документ открывался в неправильной кодировке.

Практика:
Кодировка слова "Мир" в Unicode (UTF-8):
- М = D0 9C
- и = D0 B8
- р = D1 80
(в шестнадцатеричной системе)

Кодирование в памяти:
Компьютер сохраняет текст как последовательность байтов — числовых кодов символов. Это позволяет выполнять поиск, сортировку, анализ текста.

Вывод:
Понимание кодировок символов важно для правильной работы программ, обмена данными между системами и устранения ошибок отображения текста.
